import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import shap
import matplotlib.pyplot as plt

# ä¸­æ–‡å­—å‹è¨­å®š
plt.rcParams['font.family'] = 'Microsoft JhengHei'

# è®€å–è³‡æ–™
df = pd.read_excel("processed_data (1).xlsx")

# é¸æ“‡é‡é»æ¬„ä½
features = ['æª¢å‚·ç´šæ•¸', 'pH', 'å¹´é½¡', 'å‘¼å¸æ¬¡æ•¸', 'æ„è­˜ç¨‹åº¦E', 'å¿ƒè·³', 'è¡€å£“(SBP)', 'è¡€æ°§æ¿ƒåº¦(%)']
df = df[features + ['Y']].dropna()

# ç‰¹å¾µèˆ‡ç›®æ¨™è®Šæ•¸
X = df[features]
y = df['Y']

# ç‰¹å¾µæ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ä½¿ç”¨ SMOTE å¹³è¡¡æ¨£æœ¬æ•¸
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# å»ºç«‹ XGBoost æ¨¡å‹
model = xgb.XGBClassifier(
    objective='multi:softprob',
    num_class=3,
    eval_metric='mlogloss',
    use_label_encoder=False,
    random_state=42
)
model.fit(X_train, y_train)

# é æ¸¬èˆ‡è©•ä¼°
y_pred = model.predict(X_test)
print("âœ… æº–ç¢ºç‡ï¼š", accuracy_score(y_test, y_pred))
print("ğŸ“Š åˆ†é¡å ±å‘Šï¼š\n", classification_report(y_test, y_pred))

# AUC è¨ˆç®—
y_pred_proba = model.predict_proba(X_test)
auc = roc_auc_score(pd.get_dummies(y_test), y_pred_proba, multi_class='ovr')
print("ğŸ”¥ AUCï¼š", auc)

# SHAP ç‰¹å¾µè§£é‡‹
explainer = shap.Explainer(model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, features=features)